{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eb1a753",
   "metadata": {},
   "source": [
    "# Laboratorio 8 \n",
    "    - Francis Aguilar - 22243\n",
    "    - César López - 22535\n",
    "    - Gerardo Pineda -22880\n",
    "    - Angela García -22869 \n",
    "\n",
    "\n",
    "enlace al repositorio: https://github.com/angelargd8/lab8-deep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5b32f9",
   "metadata": {},
   "source": [
    "# Task 1 - Práctica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3888b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c058fe45",
   "metadata": {},
   "source": [
    "## 1. Preparacion de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8850809c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Francis\\AppData\\Local\\Temp\\ipykernel_8764\\3001687787.py:13: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby([\"store\",\"item\"], group_keys=False).apply(_clip)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "store",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "item",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "sales",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "c893a53f-9842-41bb-82cd-e50e1032788a",
       "rows": [
        [
         "0",
         "1",
         "1",
         "2013-01-01 00:00:00",
         "331.0"
        ],
        [
         "1",
         "1",
         "1",
         "2013-02-01 00:00:00",
         "322.0"
        ],
        [
         "2",
         "1",
         "1",
         "2013-03-01 00:00:00",
         "477.0"
        ],
        [
         "3",
         "1",
         "1",
         "2013-04-01 00:00:00",
         "522.0"
        ],
        [
         "4",
         "1",
         "1",
         "2013-05-01 00:00:00",
         "531.0"
        ],
        [
         "5",
         "1",
         "1",
         "2013-06-01 00:00:00",
         "627.0"
        ],
        [
         "6",
         "1",
         "1",
         "2013-07-01 00:00:00",
         "661.0"
        ],
        [
         "7",
         "1",
         "1",
         "2013-08-01 00:00:00",
         "594.0"
        ],
        [
         "8",
         "1",
         "1",
         "2013-09-01 00:00:00",
         "519.0"
        ],
        [
         "9",
         "1",
         "1",
         "2013-10-01 00:00:00",
         "484.0"
        ],
        [
         "10",
         "1",
         "1",
         "2013-11-01 00:00:00",
         "572.0"
        ],
        [
         "11",
         "1",
         "1",
         "2013-12-01 00:00:00",
         "394.0"
        ],
        [
         "12",
         "1",
         "1",
         "2014-01-01 00:00:00",
         "437.0"
        ],
        [
         "13",
         "1",
         "1",
         "2014-02-01 00:00:00",
         "405.0"
        ],
        [
         "14",
         "1",
         "1",
         "2014-03-01 00:00:00",
         "552.0"
        ],
        [
         "15",
         "1",
         "1",
         "2014-04-01 00:00:00",
         "573.0"
        ],
        [
         "16",
         "1",
         "1",
         "2014-05-01 00:00:00",
         "651.0"
        ],
        [
         "17",
         "1",
         "1",
         "2014-06-01 00:00:00",
         "710.0"
        ],
        [
         "18",
         "1",
         "1",
         "2014-07-01 00:00:00",
         "746.0"
        ],
        [
         "19",
         "1",
         "1",
         "2014-08-01 00:00:00",
         "671.0"
        ],
        [
         "20",
         "1",
         "1",
         "2014-09-01 00:00:00",
         "566.0"
        ],
        [
         "21",
         "1",
         "1",
         "2014-10-01 00:00:00",
         "556.0"
        ],
        [
         "22",
         "1",
         "1",
         "2014-11-01 00:00:00",
         "619.0"
        ],
        [
         "23",
         "1",
         "1",
         "2014-12-01 00:00:00",
         "409.0"
        ],
        [
         "24",
         "1",
         "1",
         "2015-01-01 00:00:00",
         "428.0"
        ],
        [
         "25",
         "1",
         "1",
         "2015-02-01 00:00:00",
         "400.0"
        ],
        [
         "26",
         "1",
         "1",
         "2015-03-01 00:00:00",
         "552.0"
        ],
        [
         "27",
         "1",
         "1",
         "2015-04-01 00:00:00",
         "652.0"
        ],
        [
         "28",
         "1",
         "1",
         "2015-05-01 00:00:00",
         "711.0"
        ],
        [
         "29",
         "1",
         "1",
         "2015-06-01 00:00:00",
         "743.0"
        ],
        [
         "30",
         "1",
         "1",
         "2015-07-01 00:00:00",
         "837.0"
        ],
        [
         "31",
         "1",
         "1",
         "2015-08-01 00:00:00",
         "706.0"
        ],
        [
         "32",
         "1",
         "1",
         "2015-09-01 00:00:00",
         "676.0"
        ],
        [
         "33",
         "1",
         "1",
         "2015-10-01 00:00:00",
         "647.0"
        ],
        [
         "34",
         "1",
         "1",
         "2015-11-01 00:00:00",
         "686.0"
        ],
        [
         "35",
         "1",
         "1",
         "2015-12-01 00:00:00",
         "463.0"
        ],
        [
         "36",
         "1",
         "1",
         "2016-01-01 00:00:00",
         "449.0"
        ],
        [
         "37",
         "1",
         "1",
         "2016-02-01 00:00:00",
         "450.0"
        ],
        [
         "38",
         "1",
         "1",
         "2016-03-01 00:00:00",
         "589.0"
        ],
        [
         "39",
         "1",
         "1",
         "2016-04-01 00:00:00",
         "694.0"
        ],
        [
         "40",
         "1",
         "1",
         "2016-05-01 00:00:00",
         "781.0"
        ],
        [
         "41",
         "1",
         "1",
         "2016-06-01 00:00:00",
         "790.0"
        ],
        [
         "42",
         "1",
         "1",
         "2016-07-01 00:00:00",
         "899.0"
        ],
        [
         "43",
         "1",
         "1",
         "2016-08-01 00:00:00",
         "727.0"
        ],
        [
         "44",
         "1",
         "1",
         "2016-09-01 00:00:00",
         "709.0"
        ],
        [
         "45",
         "1",
         "1",
         "2016-10-01 00:00:00",
         "699.0"
        ],
        [
         "46",
         "1",
         "1",
         "2016-11-01 00:00:00",
         "662.0"
        ],
        [
         "47",
         "1",
         "1",
         "2016-12-01 00:00:00",
         "496.0"
        ],
        [
         "48",
         "1",
         "1",
         "2017-01-01 00:00:00",
         "485.0"
        ],
        [
         "49",
         "1",
         "1",
         "2017-02-01 00:00:00",
         "489.0"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 30000
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store</th>\n",
       "      <th>item</th>\n",
       "      <th>date</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>331.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-02-01</td>\n",
       "      <td>322.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>477.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>522.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-05-01</td>\n",
       "      <td>531.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>2866.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>2586.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>2507.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>2574.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>1987.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       store  item       date   sales\n",
       "0          1     1 2013-01-01   331.0\n",
       "1          1     1 2013-02-01   322.0\n",
       "2          1     1 2013-03-01   477.0\n",
       "3          1     1 2013-04-01   522.0\n",
       "4          1     1 2013-05-01   531.0\n",
       "...      ...   ...        ...     ...\n",
       "29995     10    50 2017-08-01  2866.0\n",
       "29996     10    50 2017-09-01  2586.0\n",
       "29997     10    50 2017-10-01  2507.0\n",
       "29998     10    50 2017-11-01  2574.0\n",
       "29999     10    50 2017-12-01  1987.0\n",
       "\n",
       "[30000 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "df = pd.read_csv(\"data/train.csv\")\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df = df.sort_values([\"store\", \"item\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "df[\"sales\"] = pd.to_numeric(df[\"sales\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "def _clip(g):\n",
    "    lo, hi = g[\"sales\"].quantile([0.01, 0.99]).values\n",
    "    g[\"sales\"] = g[\"sales\"].clip(lo, hi)\n",
    "    return g\n",
    "df = df.groupby([\"store\",\"item\"], group_keys=False).apply(_clip)\n",
    "\n",
    "\n",
    "if df[\"date\"].dt.to_period(\"M\").nunique() < df[\"date\"].nunique():\n",
    "    df[\"ym\"] = df[\"date\"].dt.to_period(\"M\").dt.to_timestamp()\n",
    "    df = (df.groupby([\"store\",\"item\",\"ym\"], as_index=False)[\"sales\"].sum()\n",
    "            .rename(columns={\"ym\":\"date\"}))\n",
    "\n",
    "all_months = pd.DataFrame({\"date\": pd.date_range(df[\"date\"].min(), df[\"date\"].max(), freq=\"MS\")})\n",
    "pairs = df[[\"store\",\"item\"]].drop_duplicates()\n",
    "panel = pairs.merge(all_months, how=\"cross\")\n",
    "df = (panel.merge(df, on=[\"store\",\"item\",\"date\"], how=\"left\")\n",
    "           .sort_values([\"store\",\"item\",\"date\"])\n",
    "           .reset_index(drop=True))\n",
    "df[\"sales\"] = df[\"sales\"].fillna(0.0)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717ea191",
   "metadata": {},
   "source": [
    "## 2.Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1f4566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIST = 18   # meses de historia por ventana\n",
    "HORIZON = 3 # meses a predecir\n",
    "months = df[\"date\"].sort_values().drop_duplicates().to_numpy()\n",
    "test_months = months[-HORIZON:]             # últimos 3 meses -> test\n",
    "val_months  = months[-(HORIZON*2): -HORIZON]      # 3 meses previos -> val\n",
    "train_months= months[:-(HORIZON*2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3a3a73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes -> X_train (17000, 18, 1) y_train (17000, 3) X_val (1500, 18, 1) y_val (1500, 3) X_test (1500, 18, 1) y_test (1500, 3)\n"
     ]
    }
   ],
   "source": [
    "def make_sequences(group_df, hist=18, horizon=-3, scaler=None):\n",
    "    g = group_df.sort_values(\"date\").copy()\n",
    "    sales = g[\"sales\"].values.reshape(-1,1)\n",
    "    s_sc = scaler.transform(sales)\n",
    "\n",
    "    X, y, dates_end = [], [], []\n",
    "    for end in range(hist, len(s_sc) - horizon + 1):\n",
    "        X.append(s_sc[end-hist:end, :])     \n",
    "        y.append(s_sc[end:end+horizon, 0])\n",
    "        dates_end.append(g[\"date\"].iloc[end+horizon-1])\n",
    "    return np.array(X), np.array(y), np.array(dates_end)\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = [], [], [], [], [], []\n",
    "pairs_index_test = []   \n",
    "\n",
    "for (store, item), g in df.groupby([\"store\",\"item\"]):\n",
    "    g = g.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    # necesitamos mínimo hist+horizon meses en train para ajustar el scaler de forma robusta\n",
    "    g_train = g[g[\"date\"].isin(train_months)].copy()\n",
    "    if len(g_train) < HIST + HORIZON:\n",
    "        continue\n",
    "\n",
    "    scaler = RobustScaler(quantile_range=(5,95))\n",
    "    scaler.fit(g_train[[\"sales\"]].values)\n",
    "\n",
    "    X_all, y_all, dates_end = make_sequences(g, HIST, HORIZON, scaler)\n",
    "\n",
    "    for Xi, yi, d_end in zip(X_all, y_all, dates_end):\n",
    "        if d_end in test_months:\n",
    "            X_test.append(Xi); y_test.append(yi); pairs_index_test.append((store,item,d_end))\n",
    "        elif d_end in val_months:\n",
    "            X_val.append(Xi);  y_val.append(yi)\n",
    "        elif d_end in train_months:\n",
    "            X_train.append(Xi); y_train.append(yi)\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_val,   y_val   = np.array(X_val),   np.array(y_val)\n",
    "X_test,  y_test  = np.array(X_test),  np.array(y_test)\n",
    "\n",
    "print(\"Shapes ->\",\n",
    "      \"X_train\", X_train.shape, \"y_train\", y_train.shape,\n",
    "      \"X_val\",   X_val.shape,   \"y_val\",   y_val.shape,\n",
    "      \"X_test\",  X_test.shape,  \"y_test\",  y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76ba57c",
   "metadata": {},
   "source": [
    "## 3 y 4  Seleccion de modelo y arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c27a700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,251</span> (83.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,251\u001b[0m (83.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,251</span> (83.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m21,251\u001b[0m (83.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LR = 1e-3\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(HIST, 1)),\n",
    "    layers.LSTM(64),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(HORIZON)\n",
    "])\n",
    "model.compile(\n",
    "    loss=\"mse\",\n",
    "    optimizer=optimizers.Adam(learning_rate=LR),\n",
    "    metrics=[tf.keras.metrics.MeanAbsoluteError(name=\"mae\")]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfdc965",
   "metadata": {},
   "source": [
    "## 5. Entrenamiento con EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01e8b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 256\n",
    "EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea431f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.0486 - mae: 0.1738 - val_loss: 0.0131 - val_mae: 0.0874 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0160 - mae: 0.1006 - val_loss: 0.0118 - val_mae: 0.0838 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0134 - mae: 0.0920 - val_loss: 0.0094 - val_mae: 0.0776 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0120 - mae: 0.0871 - val_loss: 0.0084 - val_mae: 0.0724 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0110 - mae: 0.0830 - val_loss: 0.0074 - val_mae: 0.0688 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0103 - mae: 0.0799 - val_loss: 0.0073 - val_mae: 0.0679 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0098 - mae: 0.0777 - val_loss: 0.0065 - val_mae: 0.0657 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0092 - mae: 0.0752 - val_loss: 0.0067 - val_mae: 0.0657 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0089 - mae: 0.0738 - val_loss: 0.0065 - val_mae: 0.0651 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0084 - mae: 0.0716 - val_loss: 0.0068 - val_mae: 0.0672 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0082 - mae: 0.0708 - val_loss: 0.0063 - val_mae: 0.0647 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0078 - mae: 0.0693 - val_loss: 0.0063 - val_mae: 0.0650 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0077 - mae: 0.0689 - val_loss: 0.0063 - val_mae: 0.0649 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0076 - mae: 0.0680 - val_loss: 0.0062 - val_mae: 0.0644 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0073 - mae: 0.0666 - val_loss: 0.0062 - val_mae: 0.0642 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0071 - mae: 0.0659 - val_loss: 0.0060 - val_mae: 0.0634 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0070 - mae: 0.0656 - val_loss: 0.0060 - val_mae: 0.0635 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0069 - mae: 0.0649 - val_loss: 0.0059 - val_mae: 0.0628 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0068 - mae: 0.0646 - val_loss: 0.0062 - val_mae: 0.0639 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0067 - mae: 0.0639 - val_loss: 0.0056 - val_mae: 0.0611 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0066 - mae: 0.0636 - val_loss: 0.0056 - val_mae: 0.0608 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0065 - mae: 0.0627 - val_loss: 0.0054 - val_mae: 0.0596 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0065 - mae: 0.0628 - val_loss: 0.0054 - val_mae: 0.0596 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0063 - mae: 0.0620 - val_loss: 0.0053 - val_mae: 0.0587 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0063 - mae: 0.0618 - val_loss: 0.0050 - val_mae: 0.0568 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0062 - mae: 0.0616 - val_loss: 0.0048 - val_mae: 0.0554 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0061 - mae: 0.0610 - val_loss: 0.0047 - val_mae: 0.0547 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0060 - mae: 0.0604 - val_loss: 0.0044 - val_mae: 0.0531 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0059 - mae: 0.0599 - val_loss: 0.0040 - val_mae: 0.0505 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0058 - mae: 0.0596 - val_loss: 0.0037 - val_mae: 0.0483 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0058 - mae: 0.0595 - val_loss: 0.0037 - val_mae: 0.0479 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0056 - mae: 0.0587 - val_loss: 0.0042 - val_mae: 0.0515 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0056 - mae: 0.0584 - val_loss: 0.0034 - val_mae: 0.0460 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0055 - mae: 0.0579 - val_loss: 0.0037 - val_mae: 0.0482 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0055 - mae: 0.0578 - val_loss: 0.0032 - val_mae: 0.0445 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0054 - mae: 0.0575 - val_loss: 0.0033 - val_mae: 0.0450 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0054 - mae: 0.0572 - val_loss: 0.0032 - val_mae: 0.0443 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0053 - mae: 0.0569 - val_loss: 0.0032 - val_mae: 0.0446 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0053 - mae: 0.0566 - val_loss: 0.0034 - val_mae: 0.0461 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0053 - mae: 0.0567 - val_loss: 0.0031 - val_mae: 0.0435 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0052 - mae: 0.0565 - val_loss: 0.0031 - val_mae: 0.0435 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0052 - mae: 0.0560 - val_loss: 0.0031 - val_mae: 0.0435 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0052 - mae: 0.0561 - val_loss: 0.0031 - val_mae: 0.0439 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0051 - mae: 0.0556 - val_loss: 0.0031 - val_mae: 0.0439 - learning_rate: 0.0010\n",
      "Epoch 45/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0051 - mae: 0.0558 - val_loss: 0.0030 - val_mae: 0.0433 - learning_rate: 0.0010\n",
      "Epoch 46/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0050 - mae: 0.0552 - val_loss: 0.0030 - val_mae: 0.0433 - learning_rate: 5.0000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0050 - mae: 0.0551 - val_loss: 0.0031 - val_mae: 0.0439 - learning_rate: 5.0000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0050 - mae: 0.0550 - val_loss: 0.0030 - val_mae: 0.0434 - learning_rate: 5.0000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0050 - mae: 0.0551 - val_loss: 0.0030 - val_mae: 0.0431 - learning_rate: 5.0000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0049 - mae: 0.0548 - val_loss: 0.0030 - val_mae: 0.0434 - learning_rate: 5.0000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0049 - mae: 0.0545 - val_loss: 0.0031 - val_mae: 0.0439 - learning_rate: 2.5000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0049 - mae: 0.0543 - val_loss: 0.0032 - val_mae: 0.0447 - learning_rate: 2.5000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0049 - mae: 0.0543 - val_loss: 0.0030 - val_mae: 0.0432 - learning_rate: 2.5000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0049 - mae: 0.0546 - val_loss: 0.0030 - val_mae: 0.0432 - learning_rate: 2.5000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0048 - mae: 0.0540 - val_loss: 0.0030 - val_mae: 0.0433 - learning_rate: 2.5000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0048 - mae: 0.0542 - val_loss: 0.0030 - val_mae: 0.0433 - learning_rate: 1.2500e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0049 - mae: 0.0543 - val_loss: 0.0030 - val_mae: 0.0434 - learning_rate: 1.2500e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0049 - mae: 0.0543 - val_loss: 0.0030 - val_mae: 0.0432 - learning_rate: 1.2500e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0048 - mae: 0.0540 - val_loss: 0.0030 - val_mae: 0.0431 - learning_rate: 1.2500e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0048 - mae: 0.0542 - val_loss: 0.0030 - val_mae: 0.0431 - learning_rate: 1.2500e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0048 - mae: 0.0539 - val_loss: 0.0030 - val_mae: 0.0432 - learning_rate: 6.2500e-05\n",
      "Epoch 62/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0048 - mae: 0.0541 - val_loss: 0.0030 - val_mae: 0.0431 - learning_rate: 6.2500e-05\n",
      "Epoch 63/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0047 - mae: 0.0536 - val_loss: 0.0030 - val_mae: 0.0431 - learning_rate: 6.2500e-05\n",
      "Epoch 64/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0047 - mae: 0.0538 - val_loss: 0.0030 - val_mae: 0.0432 - learning_rate: 6.2500e-05\n",
      "Epoch 65/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0048 - mae: 0.0540 - val_loss: 0.0030 - val_mae: 0.0432 - learning_rate: 6.2500e-05\n",
      "Epoch 66/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0048 - mae: 0.0541 - val_loss: 0.0030 - val_mae: 0.0429 - learning_rate: 3.1250e-05\n",
      "Epoch 67/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0048 - mae: 0.0540 - val_loss: 0.0030 - val_mae: 0.0428 - learning_rate: 3.1250e-05\n",
      "Epoch 68/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0048 - mae: 0.0540 - val_loss: 0.0029 - val_mae: 0.0427 - learning_rate: 3.1250e-05\n",
      "Epoch 69/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0048 - mae: 0.0538 - val_loss: 0.0029 - val_mae: 0.0427 - learning_rate: 3.1250e-05\n",
      "Epoch 70/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0048 - mae: 0.0540 - val_loss: 0.0030 - val_mae: 0.0428 - learning_rate: 3.1250e-05\n",
      "Epoch 71/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0048 - mae: 0.0540 - val_loss: 0.0030 - val_mae: 0.0429 - learning_rate: 3.1250e-05\n",
      "Epoch 72/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0048 - mae: 0.0540 - val_loss: 0.0030 - val_mae: 0.0430 - learning_rate: 1.5625e-05\n",
      "Epoch 73/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0047 - mae: 0.0537 - val_loss: 0.0030 - val_mae: 0.0429 - learning_rate: 1.5625e-05\n",
      "Epoch 74/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0048 - mae: 0.0538 - val_loss: 0.0030 - val_mae: 0.0429 - learning_rate: 1.5625e-05\n",
      "Epoch 75/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0047 - mae: 0.0537 - val_loss: 0.0029 - val_mae: 0.0428 - learning_rate: 1.5625e-05\n",
      "Epoch 76/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0047 - mae: 0.0536 - val_loss: 0.0029 - val_mae: 0.0428 - learning_rate: 1.5625e-05\n",
      "Epoch 77/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0047 - mae: 0.0537 - val_loss: 0.0030 - val_mae: 0.0428 - learning_rate: 1.0000e-05\n",
      "Epoch 78/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0047 - mae: 0.0534 - val_loss: 0.0029 - val_mae: 0.0427 - learning_rate: 1.0000e-05\n",
      "Epoch 79/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0048 - mae: 0.0539 - val_loss: 0.0029 - val_mae: 0.0427 - learning_rate: 1.0000e-05\n",
      "Epoch 80/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0048 - mae: 0.0539 - val_loss: 0.0030 - val_mae: 0.0428 - learning_rate: 1.0000e-05\n",
      "Epoch 81/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0048 - mae: 0.0538 - val_loss: 0.0030 - val_mae: 0.0428 - learning_rate: 1.0000e-05\n",
      "Epoch 82/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0048 - mae: 0.0540 - val_loss: 0.0029 - val_mae: 0.0427 - learning_rate: 1.0000e-05\n",
      "Epoch 83/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0048 - mae: 0.0538 - val_loss: 0.0029 - val_mae: 0.0427 - learning_rate: 1.0000e-05\n",
      "Epoch 84/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0048 - mae: 0.0539 - val_loss: 0.0029 - val_mae: 0.0427 - learning_rate: 1.0000e-05\n",
      "Epoch 85/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0048 - mae: 0.0541 - val_loss: 0.0029 - val_mae: 0.0427 - learning_rate: 1.0000e-05\n",
      "Epoch 86/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0047 - mae: 0.0537 - val_loss: 0.0029 - val_mae: 0.0426 - learning_rate: 1.0000e-05\n",
      "Epoch 87/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0048 - mae: 0.0538 - val_loss: 0.0029 - val_mae: 0.0427 - learning_rate: 1.0000e-05\n",
      "Epoch 88/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0048 - mae: 0.0537 - val_loss: 0.0029 - val_mae: 0.0427 - learning_rate: 1.0000e-05\n",
      "Epoch 89/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0048 - mae: 0.0537 - val_loss: 0.0030 - val_mae: 0.0428 - learning_rate: 1.0000e-05\n",
      "Epoch 90/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0048 - mae: 0.0539 - val_loss: 0.0029 - val_mae: 0.0427 - learning_rate: 1.0000e-05\n",
      "Epoch 91/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0048 - mae: 0.0540 - val_loss: 0.0029 - val_mae: 0.0427 - learning_rate: 1.0000e-05\n",
      "Epoch 92/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0047 - mae: 0.0536 - val_loss: 0.0029 - val_mae: 0.0427 - learning_rate: 1.0000e-05\n",
      "Epoch 93/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0047 - mae: 0.0537 - val_loss: 0.0030 - val_mae: 0.0428 - learning_rate: 1.0000e-05\n",
      "Epoch 94/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0047 - mae: 0.0536 - val_loss: 0.0029 - val_mae: 0.0427 - learning_rate: 1.0000e-05\n",
      "Epoch 95/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0048 - mae: 0.0539 - val_loss: 0.0030 - val_mae: 0.0428 - learning_rate: 1.0000e-05\n",
      "Epoch 96/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0047 - mae: 0.0535 - val_loss: 0.0029 - val_mae: 0.0427 - learning_rate: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "es  = callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "rlr = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-5)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH,\n",
    "    callbacks=[es, rlr],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5175b3e1",
   "metadata": {},
   "source": [
    "## 6. Evaluación del modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a7bf075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0027 - mae: 0.0404\n",
      "Loss (MSE): 0.0027\n",
      "MAE: 0.0404\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "Métricas de evaluación:\n",
      "MAE  = 0.0404\n",
      "MSE  = 0.0027\n",
      "RMSE = 0.0521\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Evaluación directa con Keras\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Loss (MSE): {test_loss:.4f}\")\n",
    "print(f\"MAE: {test_mae:.4f}\")\n",
    "\n",
    "# Predicciones sobre el conjunto de test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Cálculo de métricas adicionales\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"\\nMétricas de evaluación:\")\n",
    "print(f\"MAE  = {mae:.4f}\")\n",
    "print(f\"MSE  = {mse:.4f}\")\n",
    "print(f\"RMSE = {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9235b41d",
   "metadata": {},
   "source": [
    "## 7. Ajuste de hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d33a7c0",
   "metadata": {},
   "source": [
    "# Task 2 - Teoría\n",
    "\n",
    "### 1. ¿Cuál es el problema del gradiente de fuga en las redes LSTM y cómo afecta la efectividad de LSTM para el pronóstico de series temporales? \n",
    "El problema del gradiente de fuga en las redes LSTM, es que aunque las LSTM están diseñadas para mitigarlo no son inmunes, ya que en LSTM el estado de la celda va de esta manera:\n",
    "\n",
    "$$\n",
    "c_t = f_t \\odot c_{t-1} + i_t \\odot g_t\n",
    "$$\n",
    "\n",
    "Y en la retropropagación a través del tiempo hace que la gradiente hacia:\n",
    "$$\n",
    "c_{t-1}\n",
    "$$\n",
    "\n",
    "se multiplique por el producto de las compuertas del olvido, entonces el resultado del producto tiende a 0 cuando la secuencia es larga o las compuertas se cierran más, entonces el gradiente se desvanece y si fuera mayor que 1 explotaría. Y eso pasa en la práctuca ya que se puede dar la saturación de sigmoides, sesgo del olvido mal iniciado, secuencias largas y normalización o escala pobre de entradas. Y esto afecta al pronóstico, porque no capta dependencias de largo plazo, tiene predicciones que sub-reaccionan a las señales antiguas, tiene una convergencia lenta y es inestable, ya que a varios pasos el error crece y el modelo olvida el contexto útil.\n",
    "\n",
    "\n",
    "### 2. ¿Cómo se aborda la estacionalidad en los datos de series temporales cuando se utilizan LSTM para realizar pronósticos y qué papel juega la diferenciación en el proceso? \n",
    "\n",
    "El manejo de estacionalidad en LSTM se puede modelar la estacionalidad como caracteristicas o al quitarla y recomponerla. Ya que, la diferenciación lo que hacec es estabilizar y facilitar el aprendizaje, pero requiere de reintegrar y puede sobreactuar si la estacionalidad no es rígida. En el caso que la estacionalidad sea fuerte o regular es mejor deseasonalizarla, quitarla al entrenar el LSTM sobre la serie limpua y reaplicar al estacionalidad al final. Mientras que si es informativa para el modelo, es mejor agregarlas como features exógenas.\n",
    "\n",
    "\n",
    "\n",
    "### 3. ¿Cuál es el concepto de \"tamaño de ventana\" en el pronóstico de series temporales con LSTM y cómo afecta la elección del tamaño de ventana a la capacidad del modelo para capturar patrones a corto y largo plazo? \n",
    "\n",
    "El concepto de tamaño de ventana o tambien llamado lookback o W, es cuantos pasos del pasado se le entrega al modelo para predecir el futuro. Y el W si es una ventana corta o pequeña, tiende a patrones de corto plazo, entonces este aprende rápidamente señales locales, pero no ve ciclos largos. Mientras que si es una ventana larga o grande, tiende a patrones de largo plazo, estos pueden capturar tendencias y estacionalidades largas si están presentes en la ventana, tiene parámetros más efectivos, mayor varianza/overfitting, y el entrenamiento es más lengo. Por el desvanecimiento de gradientes si la información útil está muy lejos el LSTM no puede aprovechar todo. Por lo tanto para evaluar el tamaño de la ventana y saber como ajustarla es imporante analizar la serie y su ciclo. Si la H es grande, los datos limitados para evitar W demasiado grandes y si tiene múltiples ciclos. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b528321c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
