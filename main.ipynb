{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eb1a753",
   "metadata": {},
   "source": [
    "# Laboratorio 8 \n",
    "    - Francis Aguilar - 22243\n",
    "    - César López - 22535\n",
    "    - Gerardo Pineda -22880\n",
    "    - Angela García -22869 \n",
    "\n",
    "\n",
    "enlace al repositorio: https://github.com/angelargd8/lab8-deep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5b32f9",
   "metadata": {},
   "source": [
    "# Task 1 - Práctica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3888b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c058fe45",
   "metadata": {},
   "source": [
    "## 1. Preparacion de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8850809c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI\\AppData\\Local\\Temp\\ipykernel_33032\\3001687787.py:13: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby([\"store\",\"item\"], group_keys=False).apply(_clip)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store</th>\n",
       "      <th>item</th>\n",
       "      <th>date</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>331.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-02-01</td>\n",
       "      <td>322.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>477.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>522.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-05-01</td>\n",
       "      <td>531.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>2866.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>2586.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>2507.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>2574.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>1987.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       store  item       date   sales\n",
       "0          1     1 2013-01-01   331.0\n",
       "1          1     1 2013-02-01   322.0\n",
       "2          1     1 2013-03-01   477.0\n",
       "3          1     1 2013-04-01   522.0\n",
       "4          1     1 2013-05-01   531.0\n",
       "...      ...   ...        ...     ...\n",
       "29995     10    50 2017-08-01  2866.0\n",
       "29996     10    50 2017-09-01  2586.0\n",
       "29997     10    50 2017-10-01  2507.0\n",
       "29998     10    50 2017-11-01  2574.0\n",
       "29999     10    50 2017-12-01  1987.0\n",
       "\n",
       "[30000 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "df = pd.read_csv(\"data/train.csv\")\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df = df.sort_values([\"store\", \"item\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "df[\"sales\"] = pd.to_numeric(df[\"sales\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "def _clip(g):\n",
    "    lo, hi = g[\"sales\"].quantile([0.01, 0.99]).values\n",
    "    g[\"sales\"] = g[\"sales\"].clip(lo, hi)\n",
    "    return g\n",
    "df = df.groupby([\"store\",\"item\"], group_keys=False).apply(_clip)\n",
    "\n",
    "\n",
    "if df[\"date\"].dt.to_period(\"M\").nunique() < df[\"date\"].nunique():\n",
    "    df[\"ym\"] = df[\"date\"].dt.to_period(\"M\").dt.to_timestamp()\n",
    "    df = (df.groupby([\"store\",\"item\",\"ym\"], as_index=False)[\"sales\"].sum()\n",
    "            .rename(columns={\"ym\":\"date\"}))\n",
    "\n",
    "all_months = pd.DataFrame({\"date\": pd.date_range(df[\"date\"].min(), df[\"date\"].max(), freq=\"MS\")})\n",
    "pairs = df[[\"store\",\"item\"]].drop_duplicates()\n",
    "panel = pairs.merge(all_months, how=\"cross\")\n",
    "df = (panel.merge(df, on=[\"store\",\"item\",\"date\"], how=\"left\")\n",
    "           .sort_values([\"store\",\"item\",\"date\"])\n",
    "           .reset_index(drop=True))\n",
    "df[\"sales\"] = df[\"sales\"].fillna(0.0)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717ea191",
   "metadata": {},
   "source": [
    "## 2.Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1f4566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIST = 18   # meses de historia por ventana\n",
    "HORIZON = 3 # meses a predecir\n",
    "months = df[\"date\"].sort_values().drop_duplicates().to_numpy()\n",
    "test_months = months[-HORIZON:]             # últimos 3 meses -> test\n",
    "val_months  = months[-(HORIZON*2): -HORIZON]      # 3 meses previos -> val\n",
    "train_months= months[:-(HORIZON*2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3a3a73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes -> X_train (17000, 18, 1) y_train (17000, 3) X_val (1500, 18, 1) y_val (1500, 3) X_test (1500, 18, 1) y_test (1500, 3)\n"
     ]
    }
   ],
   "source": [
    "def make_sequences(group_df, hist=18, horizon=-3, scaler=None):\n",
    "    g = group_df.sort_values(\"date\").copy()\n",
    "    sales = g[\"sales\"].values.reshape(-1,1)\n",
    "    s_sc = scaler.transform(sales)\n",
    "\n",
    "    X, y, dates_end = [], [], []\n",
    "    for end in range(hist, len(s_sc) - horizon + 1):\n",
    "        X.append(s_sc[end-hist:end, :])     \n",
    "        y.append(s_sc[end:end+horizon, 0])\n",
    "        dates_end.append(g[\"date\"].iloc[end+horizon-1])\n",
    "    return np.array(X), np.array(y), np.array(dates_end)\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = [], [], [], [], [], []\n",
    "pairs_index_test = []   \n",
    "\n",
    "for (store, item), g in df.groupby([\"store\",\"item\"]):\n",
    "    g = g.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    # necesitamos mínimo hist+horizon meses en train para ajustar el scaler de forma robusta\n",
    "    g_train = g[g[\"date\"].isin(train_months)].copy()\n",
    "    if len(g_train) < HIST + HORIZON:\n",
    "        continue\n",
    "\n",
    "    scaler = RobustScaler(quantile_range=(5,95))\n",
    "    scaler.fit(g_train[[\"sales\"]].values)\n",
    "\n",
    "    X_all, y_all, dates_end = make_sequences(g, HIST, HORIZON, scaler)\n",
    "\n",
    "    for Xi, yi, d_end in zip(X_all, y_all, dates_end):\n",
    "        if d_end in test_months:\n",
    "            X_test.append(Xi); y_test.append(yi); pairs_index_test.append((store,item,d_end))\n",
    "        elif d_end in val_months:\n",
    "            X_val.append(Xi);  y_val.append(yi)\n",
    "        elif d_end in train_months:\n",
    "            X_train.append(Xi); y_train.append(yi)\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_val,   y_val   = np.array(X_val),   np.array(y_val)\n",
    "X_test,  y_test  = np.array(X_test),  np.array(y_test)\n",
    "\n",
    "print(\"Shapes ->\",\n",
    "      \"X_train\", X_train.shape, \"y_train\", y_train.shape,\n",
    "      \"X_val\",   X_val.shape,   \"y_val\",   y_val.shape,\n",
    "      \"X_test\",  X_test.shape,  \"y_test\",  y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76ba57c",
   "metadata": {},
   "source": [
    "## 3 y 4  Seleccion de modelo y arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c27a700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,251</span> (83.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,251\u001b[0m (83.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,251</span> (83.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m21,251\u001b[0m (83.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LR = 1e-3\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(HIST, 1)),\n",
    "    layers.LSTM(64),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(HORIZON)\n",
    "])\n",
    "model.compile(\n",
    "    loss=\"mse\",\n",
    "    optimizer=optimizers.Adam(learning_rate=LR),\n",
    "    metrics=[tf.keras.metrics.MeanAbsoluteError(name=\"mae\")]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfdc965",
   "metadata": {},
   "source": [
    "## 5. Entrenamiento con EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01e8b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 256\n",
    "EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea431f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.0451 - mae: 0.1670 - val_loss: 0.0151 - val_mae: 0.0933 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0160 - mae: 0.1007 - val_loss: 0.0120 - val_mae: 0.0824 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0134 - mae: 0.0920 - val_loss: 0.0082 - val_mae: 0.0719 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0118 - mae: 0.0862 - val_loss: 0.0078 - val_mae: 0.0693 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0108 - mae: 0.0821 - val_loss: 0.0068 - val_mae: 0.0647 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0099 - mae: 0.0784 - val_loss: 0.0062 - val_mae: 0.0623 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0093 - mae: 0.0760 - val_loss: 0.0067 - val_mae: 0.0658 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0088 - mae: 0.0740 - val_loss: 0.0057 - val_mae: 0.0603 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0085 - mae: 0.0726 - val_loss: 0.0059 - val_mae: 0.0609 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0082 - mae: 0.0708 - val_loss: 0.0058 - val_mae: 0.0602 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0080 - mae: 0.0700 - val_loss: 0.0056 - val_mae: 0.0597 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0077 - mae: 0.0686 - val_loss: 0.0057 - val_mae: 0.0602 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0076 - mae: 0.0682 - val_loss: 0.0050 - val_mae: 0.0562 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0073 - mae: 0.0669 - val_loss: 0.0050 - val_mae: 0.0560 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0072 - mae: 0.0667 - val_loss: 0.0045 - val_mae: 0.0531 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0071 - mae: 0.0657 - val_loss: 0.0045 - val_mae: 0.0533 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0068 - mae: 0.0644 - val_loss: 0.0045 - val_mae: 0.0527 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0066 - mae: 0.0636 - val_loss: 0.0042 - val_mae: 0.0506 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0066 - mae: 0.0634 - val_loss: 0.0038 - val_mae: 0.0483 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0064 - mae: 0.0626 - val_loss: 0.0038 - val_mae: 0.0484 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0063 - mae: 0.0622 - val_loss: 0.0034 - val_mae: 0.0457 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0062 - mae: 0.0614 - val_loss: 0.0034 - val_mae: 0.0455 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0062 - mae: 0.0610 - val_loss: 0.0033 - val_mae: 0.0447 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0060 - mae: 0.0601 - val_loss: 0.0032 - val_mae: 0.0438 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0059 - mae: 0.0600 - val_loss: 0.0031 - val_mae: 0.0434 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0058 - mae: 0.0597 - val_loss: 0.0034 - val_mae: 0.0454 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0057 - mae: 0.0589 - val_loss: 0.0030 - val_mae: 0.0427 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0057 - mae: 0.0588 - val_loss: 0.0031 - val_mae: 0.0432 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0056 - mae: 0.0585 - val_loss: 0.0031 - val_mae: 0.0438 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0055 - mae: 0.0580 - val_loss: 0.0030 - val_mae: 0.0425 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0055 - mae: 0.0579 - val_loss: 0.0030 - val_mae: 0.0430 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0055 - mae: 0.0575 - val_loss: 0.0032 - val_mae: 0.0446 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0053 - mae: 0.0568 - val_loss: 0.0030 - val_mae: 0.0427 - learning_rate: 5.0000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0053 - mae: 0.0567 - val_loss: 0.0030 - val_mae: 0.0425 - learning_rate: 5.0000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0053 - mae: 0.0567 - val_loss: 0.0029 - val_mae: 0.0417 - learning_rate: 5.0000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0053 - mae: 0.0564 - val_loss: 0.0029 - val_mae: 0.0421 - learning_rate: 5.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0052 - mae: 0.0562 - val_loss: 0.0031 - val_mae: 0.0433 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0052 - mae: 0.0561 - val_loss: 0.0029 - val_mae: 0.0423 - learning_rate: 5.0000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0052 - mae: 0.0562 - val_loss: 0.0031 - val_mae: 0.0436 - learning_rate: 5.0000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0052 - mae: 0.0560 - val_loss: 0.0029 - val_mae: 0.0419 - learning_rate: 5.0000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0051 - mae: 0.0556 - val_loss: 0.0030 - val_mae: 0.0430 - learning_rate: 2.5000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0051 - mae: 0.0555 - val_loss: 0.0029 - val_mae: 0.0419 - learning_rate: 2.5000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0051 - mae: 0.0558 - val_loss: 0.0030 - val_mae: 0.0428 - learning_rate: 2.5000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0051 - mae: 0.0556 - val_loss: 0.0029 - val_mae: 0.0419 - learning_rate: 2.5000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0051 - mae: 0.0555 - val_loss: 0.0029 - val_mae: 0.0424 - learning_rate: 2.5000e-04\n"
     ]
    }
   ],
   "source": [
    "es  = callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "rlr = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-5)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH,\n",
    "    callbacks=[es, rlr],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5175b3e1",
   "metadata": {},
   "source": [
    "## 6. Evaluación del modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a7bf075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0039 - mae: 0.0478\n",
      "Loss (MSE): 0.0039\n",
      "MAE: 0.0478\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\n",
      "Métricas de evaluación:\n",
      "MAE  = 0.0478\n",
      "MSE  = 0.0039\n",
      "RMSE = 0.0624\n",
      "R² = 0.6523\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Evaluación directa con Keras\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Loss (MSE): {test_loss:.4f}\")\n",
    "print(f\"MAE: {test_mae:.4f}\")\n",
    "\n",
    "# Predicciones sobre el conjunto de test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Cálculo de métricas adicionales\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nMétricas de evaluación:\")\n",
    "print(f\"MAE  = {mae:.4f}\")\n",
    "print(f\"MSE  = {mse:.4f}\")\n",
    "print(f\"RMSE = {rmse:.4f}\")\n",
    "print(f\"R² = {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aff56fc",
   "metadata": {},
   "source": [
    "Los resultados del modelo indican un desempeño bastante bueno en términos de error absoluto y cuadrático: la **MAE de 0.0536** sugiere que, en promedio, las predicciones se desvían aproximadamente 0.054 unidades de los valores reales, mientras que el **RMSE de 0.0678** confirma que los errores más grandes no son excesivos, mostrando una distribución de errores relativamente uniforme. La **MSE de 0.0046** respalda esta observación al reflejar un error cuadrático pequeño. Sin embargo, el **R² de 0.5155** indica que el modelo explica alrededor del 51% de la variabilidad de los datos, lo que significa que aún existe una parte considerable de la variación que no se captura. En conjunto, el modelo predice con precisión moderada, con errores bajos, pero podría mejorar en la capacidad de capturar toda la dinámica de los datos. También puede ser indice de sobreajuste\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9235b41d",
   "metadata": {},
   "source": [
    "## 7. Ajuste de hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bfa2911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(units=64, stacked=False, dropout=0.0, dense_units=64, lr=1e-3, output_size=None):\n",
    "    if output_size is None:\n",
    "        output_size = HORIZON\n",
    "\n",
    "    m = models.Sequential()\n",
    "    m.add(layers.Input(shape=(HIST, 1)))\n",
    "\n",
    "    if stacked:\n",
    "        m.add(layers.LSTM(units, return_sequences=True))\n",
    "        if dropout > 0:\n",
    "            m.add(layers.Dropout(dropout))\n",
    "        m.add(layers.LSTM(units))\n",
    "    else:\n",
    "        m.add(layers.LSTM(units))\n",
    "\n",
    "    if dropout > 0:\n",
    "        m.add(layers.Dropout(dropout))\n",
    "\n",
    "    if dense_units and dense_units > 0:\n",
    "        m.add(layers.Dense(dense_units, activation=\"relu\"))\n",
    "\n",
    "    m.add(layers.Dense(output_size, activation=\"linear\"))\n",
    "\n",
    "    opt = optimizers.Adam(learning_rate=lr)\n",
    "    m.compile(optimizer=opt, loss=\"mse\")\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bf63cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probando 144 combinaciones...\n",
      "\n",
      "[1/144] {'units': 32, 'stacked': False, 'dropout': 0.0, 'dense_units': 64, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[2/144] {'units': 32, 'stacked': False, 'dropout': 0.0, 'dense_units': 64, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[3/144] {'units': 32, 'stacked': False, 'dropout': 0.0, 'dense_units': 64, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[4/144] {'units': 32, 'stacked': False, 'dropout': 0.0, 'dense_units': 64, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[5/144] {'units': 32, 'stacked': False, 'dropout': 0.0, 'dense_units': 128, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[6/144] {'units': 32, 'stacked': False, 'dropout': 0.0, 'dense_units': 128, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[7/144] {'units': 32, 'stacked': False, 'dropout': 0.0, 'dense_units': 128, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[8/144] {'units': 32, 'stacked': False, 'dropout': 0.0, 'dense_units': 128, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[9/144] {'units': 32, 'stacked': False, 'dropout': 0.2, 'dense_units': 64, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[10/144] {'units': 32, 'stacked': False, 'dropout': 0.2, 'dense_units': 64, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[11/144] {'units': 32, 'stacked': False, 'dropout': 0.2, 'dense_units': 64, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[12/144] {'units': 32, 'stacked': False, 'dropout': 0.2, 'dense_units': 64, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[13/144] {'units': 32, 'stacked': False, 'dropout': 0.2, 'dense_units': 128, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[14/144] {'units': 32, 'stacked': False, 'dropout': 0.2, 'dense_units': 128, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[15/144] {'units': 32, 'stacked': False, 'dropout': 0.2, 'dense_units': 128, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[16/144] {'units': 32, 'stacked': False, 'dropout': 0.2, 'dense_units': 128, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[17/144] {'units': 32, 'stacked': False, 'dropout': 0.4, 'dense_units': 64, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[18/144] {'units': 32, 'stacked': False, 'dropout': 0.4, 'dense_units': 64, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[19/144] {'units': 32, 'stacked': False, 'dropout': 0.4, 'dense_units': 64, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[20/144] {'units': 32, 'stacked': False, 'dropout': 0.4, 'dense_units': 64, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[21/144] {'units': 32, 'stacked': False, 'dropout': 0.4, 'dense_units': 128, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[22/144] {'units': 32, 'stacked': False, 'dropout': 0.4, 'dense_units': 128, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[23/144] {'units': 32, 'stacked': False, 'dropout': 0.4, 'dense_units': 128, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[24/144] {'units': 32, 'stacked': False, 'dropout': 0.4, 'dense_units': 128, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[25/144] {'units': 32, 'stacked': True, 'dropout': 0.0, 'dense_units': 64, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[26/144] {'units': 32, 'stacked': True, 'dropout': 0.0, 'dense_units': 64, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[27/144] {'units': 32, 'stacked': True, 'dropout': 0.0, 'dense_units': 64, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[28/144] {'units': 32, 'stacked': True, 'dropout': 0.0, 'dense_units': 64, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[29/144] {'units': 32, 'stacked': True, 'dropout': 0.0, 'dense_units': 128, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[30/144] {'units': 32, 'stacked': True, 'dropout': 0.0, 'dense_units': 128, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[31/144] {'units': 32, 'stacked': True, 'dropout': 0.0, 'dense_units': 128, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[32/144] {'units': 32, 'stacked': True, 'dropout': 0.0, 'dense_units': 128, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[33/144] {'units': 32, 'stacked': True, 'dropout': 0.2, 'dense_units': 64, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[34/144] {'units': 32, 'stacked': True, 'dropout': 0.2, 'dense_units': 64, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[35/144] {'units': 32, 'stacked': True, 'dropout': 0.2, 'dense_units': 64, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[36/144] {'units': 32, 'stacked': True, 'dropout': 0.2, 'dense_units': 64, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[37/144] {'units': 32, 'stacked': True, 'dropout': 0.2, 'dense_units': 128, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[38/144] {'units': 32, 'stacked': True, 'dropout': 0.2, 'dense_units': 128, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[39/144] {'units': 32, 'stacked': True, 'dropout': 0.2, 'dense_units': 128, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[40/144] {'units': 32, 'stacked': True, 'dropout': 0.2, 'dense_units': 128, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[41/144] {'units': 32, 'stacked': True, 'dropout': 0.4, 'dense_units': 64, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[42/144] {'units': 32, 'stacked': True, 'dropout': 0.4, 'dense_units': 64, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[43/144] {'units': 32, 'stacked': True, 'dropout': 0.4, 'dense_units': 64, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[44/144] {'units': 32, 'stacked': True, 'dropout': 0.4, 'dense_units': 64, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[45/144] {'units': 32, 'stacked': True, 'dropout': 0.4, 'dense_units': 128, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[46/144] {'units': 32, 'stacked': True, 'dropout': 0.4, 'dense_units': 128, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[47/144] {'units': 32, 'stacked': True, 'dropout': 0.4, 'dense_units': 128, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[48/144] {'units': 32, 'stacked': True, 'dropout': 0.4, 'dense_units': 128, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[49/144] {'units': 64, 'stacked': False, 'dropout': 0.0, 'dense_units': 64, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[50/144] {'units': 64, 'stacked': False, 'dropout': 0.0, 'dense_units': 64, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[51/144] {'units': 64, 'stacked': False, 'dropout': 0.0, 'dense_units': 64, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[52/144] {'units': 64, 'stacked': False, 'dropout': 0.0, 'dense_units': 64, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[53/144] {'units': 64, 'stacked': False, 'dropout': 0.0, 'dense_units': 128, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[54/144] {'units': 64, 'stacked': False, 'dropout': 0.0, 'dense_units': 128, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[55/144] {'units': 64, 'stacked': False, 'dropout': 0.0, 'dense_units': 128, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[56/144] {'units': 64, 'stacked': False, 'dropout': 0.0, 'dense_units': 128, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[57/144] {'units': 64, 'stacked': False, 'dropout': 0.2, 'dense_units': 64, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[58/144] {'units': 64, 'stacked': False, 'dropout': 0.2, 'dense_units': 64, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[59/144] {'units': 64, 'stacked': False, 'dropout': 0.2, 'dense_units': 64, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[60/144] {'units': 64, 'stacked': False, 'dropout': 0.2, 'dense_units': 64, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[61/144] {'units': 64, 'stacked': False, 'dropout': 0.2, 'dense_units': 128, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[62/144] {'units': 64, 'stacked': False, 'dropout': 0.2, 'dense_units': 128, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[63/144] {'units': 64, 'stacked': False, 'dropout': 0.2, 'dense_units': 128, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[64/144] {'units': 64, 'stacked': False, 'dropout': 0.2, 'dense_units': 128, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[65/144] {'units': 64, 'stacked': False, 'dropout': 0.4, 'dense_units': 64, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[66/144] {'units': 64, 'stacked': False, 'dropout': 0.4, 'dense_units': 64, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[67/144] {'units': 64, 'stacked': False, 'dropout': 0.4, 'dense_units': 64, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[68/144] {'units': 64, 'stacked': False, 'dropout': 0.4, 'dense_units': 64, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[69/144] {'units': 64, 'stacked': False, 'dropout': 0.4, 'dense_units': 128, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[70/144] {'units': 64, 'stacked': False, 'dropout': 0.4, 'dense_units': 128, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[71/144] {'units': 64, 'stacked': False, 'dropout': 0.4, 'dense_units': 128, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[72/144] {'units': 64, 'stacked': False, 'dropout': 0.4, 'dense_units': 128, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[73/144] {'units': 64, 'stacked': True, 'dropout': 0.0, 'dense_units': 64, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[74/144] {'units': 64, 'stacked': True, 'dropout': 0.0, 'dense_units': 64, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[75/144] {'units': 64, 'stacked': True, 'dropout': 0.0, 'dense_units': 64, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[76/144] {'units': 64, 'stacked': True, 'dropout': 0.0, 'dense_units': 64, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[77/144] {'units': 64, 'stacked': True, 'dropout': 0.0, 'dense_units': 128, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[78/144] {'units': 64, 'stacked': True, 'dropout': 0.0, 'dense_units': 128, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[79/144] {'units': 64, 'stacked': True, 'dropout': 0.0, 'dense_units': 128, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[80/144] {'units': 64, 'stacked': True, 'dropout': 0.0, 'dense_units': 128, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[81/144] {'units': 64, 'stacked': True, 'dropout': 0.2, 'dense_units': 64, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[82/144] {'units': 64, 'stacked': True, 'dropout': 0.2, 'dense_units': 64, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[83/144] {'units': 64, 'stacked': True, 'dropout': 0.2, 'dense_units': 64, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[84/144] {'units': 64, 'stacked': True, 'dropout': 0.2, 'dense_units': 64, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[85/144] {'units': 64, 'stacked': True, 'dropout': 0.2, 'dense_units': 128, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[86/144] {'units': 64, 'stacked': True, 'dropout': 0.2, 'dense_units': 128, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[87/144] {'units': 64, 'stacked': True, 'dropout': 0.2, 'dense_units': 128, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[88/144] {'units': 64, 'stacked': True, 'dropout': 0.2, 'dense_units': 128, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[89/144] {'units': 64, 'stacked': True, 'dropout': 0.4, 'dense_units': 64, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[90/144] {'units': 64, 'stacked': True, 'dropout': 0.4, 'dense_units': 64, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[91/144] {'units': 64, 'stacked': True, 'dropout': 0.4, 'dense_units': 64, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[92/144] {'units': 64, 'stacked': True, 'dropout': 0.4, 'dense_units': 64, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[93/144] {'units': 64, 'stacked': True, 'dropout': 0.4, 'dense_units': 128, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[94/144] {'units': 64, 'stacked': True, 'dropout': 0.4, 'dense_units': 128, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[95/144] {'units': 64, 'stacked': True, 'dropout': 0.4, 'dense_units': 128, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[96/144] {'units': 64, 'stacked': True, 'dropout': 0.4, 'dense_units': 128, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[97/144] {'units': 128, 'stacked': False, 'dropout': 0.0, 'dense_units': 64, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[98/144] {'units': 128, 'stacked': False, 'dropout': 0.0, 'dense_units': 64, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[99/144] {'units': 128, 'stacked': False, 'dropout': 0.0, 'dense_units': 64, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[100/144] {'units': 128, 'stacked': False, 'dropout': 0.0, 'dense_units': 64, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[101/144] {'units': 128, 'stacked': False, 'dropout': 0.0, 'dense_units': 128, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[102/144] {'units': 128, 'stacked': False, 'dropout': 0.0, 'dense_units': 128, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[103/144] {'units': 128, 'stacked': False, 'dropout': 0.0, 'dense_units': 128, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[104/144] {'units': 128, 'stacked': False, 'dropout': 0.0, 'dense_units': 128, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[105/144] {'units': 128, 'stacked': False, 'dropout': 0.2, 'dense_units': 64, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[106/144] {'units': 128, 'stacked': False, 'dropout': 0.2, 'dense_units': 64, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[107/144] {'units': 128, 'stacked': False, 'dropout': 0.2, 'dense_units': 64, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[108/144] {'units': 128, 'stacked': False, 'dropout': 0.2, 'dense_units': 64, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[109/144] {'units': 128, 'stacked': False, 'dropout': 0.2, 'dense_units': 128, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[110/144] {'units': 128, 'stacked': False, 'dropout': 0.2, 'dense_units': 128, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[111/144] {'units': 128, 'stacked': False, 'dropout': 0.2, 'dense_units': 128, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[112/144] {'units': 128, 'stacked': False, 'dropout': 0.2, 'dense_units': 128, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[113/144] {'units': 128, 'stacked': False, 'dropout': 0.4, 'dense_units': 64, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[114/144] {'units': 128, 'stacked': False, 'dropout': 0.4, 'dense_units': 64, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[115/144] {'units': 128, 'stacked': False, 'dropout': 0.4, 'dense_units': 64, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[116/144] {'units': 128, 'stacked': False, 'dropout': 0.4, 'dense_units': 64, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[117/144] {'units': 128, 'stacked': False, 'dropout': 0.4, 'dense_units': 128, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[118/144] {'units': 128, 'stacked': False, 'dropout': 0.4, 'dense_units': 128, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[119/144] {'units': 128, 'stacked': False, 'dropout': 0.4, 'dense_units': 128, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[120/144] {'units': 128, 'stacked': False, 'dropout': 0.4, 'dense_units': 128, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[121/144] {'units': 128, 'stacked': True, 'dropout': 0.0, 'dense_units': 64, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[122/144] {'units': 128, 'stacked': True, 'dropout': 0.0, 'dense_units': 64, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[123/144] {'units': 128, 'stacked': True, 'dropout': 0.0, 'dense_units': 64, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[124/144] {'units': 128, 'stacked': True, 'dropout': 0.0, 'dense_units': 64, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[125/144] {'units': 128, 'stacked': True, 'dropout': 0.0, 'dense_units': 128, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[126/144] {'units': 128, 'stacked': True, 'dropout': 0.0, 'dense_units': 128, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[127/144] {'units': 128, 'stacked': True, 'dropout': 0.0, 'dense_units': 128, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[128/144] {'units': 128, 'stacked': True, 'dropout': 0.0, 'dense_units': 128, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[129/144] {'units': 128, 'stacked': True, 'dropout': 0.2, 'dense_units': 64, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[130/144] {'units': 128, 'stacked': True, 'dropout': 0.2, 'dense_units': 64, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[131/144] {'units': 128, 'stacked': True, 'dropout': 0.2, 'dense_units': 64, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[132/144] {'units': 128, 'stacked': True, 'dropout': 0.2, 'dense_units': 64, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[133/144] {'units': 128, 'stacked': True, 'dropout': 0.2, 'dense_units': 128, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[134/144] {'units': 128, 'stacked': True, 'dropout': 0.2, 'dense_units': 128, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[135/144] {'units': 128, 'stacked': True, 'dropout': 0.2, 'dense_units': 128, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[136/144] {'units': 128, 'stacked': True, 'dropout': 0.2, 'dense_units': 128, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[137/144] {'units': 128, 'stacked': True, 'dropout': 0.4, 'dense_units': 64, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[138/144] {'units': 128, 'stacked': True, 'dropout': 0.4, 'dense_units': 64, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[139/144] {'units': 128, 'stacked': True, 'dropout': 0.4, 'dense_units': 64, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[140/144] {'units': 128, 'stacked': True, 'dropout': 0.4, 'dense_units': 64, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[141/144] {'units': 128, 'stacked': True, 'dropout': 0.4, 'dense_units': 128, 'lr': 0.001, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[142/144] {'units': 128, 'stacked': True, 'dropout': 0.4, 'dense_units': 128, 'lr': 0.001, 'batch': 256, 'epochs': 60}\n",
      "\n",
      "[143/144] {'units': 128, 'stacked': True, 'dropout': 0.4, 'dense_units': 128, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "\n",
      "[144/144] {'units': 128, 'stacked': True, 'dropout': 0.4, 'dense_units': 128, 'lr': 0.0005, 'batch': 256, 'epochs': 60}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>stacked</th>\n",
       "      <th>dropout</th>\n",
       "      <th>dense_units</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch</th>\n",
       "      <th>epochs</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>time_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>0.2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>128</td>\n",
       "      <td>60</td>\n",
       "      <td>0.026194</td>\n",
       "      <td>0.020804</td>\n",
       "      <td>6.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>0.2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>0.026305</td>\n",
       "      <td>0.020811</td>\n",
       "      <td>5.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>0.025342</td>\n",
       "      <td>0.020820</td>\n",
       "      <td>4.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>False</td>\n",
       "      <td>0.2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>0.026378</td>\n",
       "      <td>0.020838</td>\n",
       "      <td>7.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>0.2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>128</td>\n",
       "      <td>60</td>\n",
       "      <td>0.025975</td>\n",
       "      <td>0.020841</td>\n",
       "      <td>8.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>128</td>\n",
       "      <td>False</td>\n",
       "      <td>0.2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>0.025820</td>\n",
       "      <td>0.020842</td>\n",
       "      <td>22.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>0.2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>0.026573</td>\n",
       "      <td>0.020849</td>\n",
       "      <td>4.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>64</td>\n",
       "      <td>False</td>\n",
       "      <td>0.4</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>0.027030</td>\n",
       "      <td>0.020854</td>\n",
       "      <td>9.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>0.4</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>0.028095</td>\n",
       "      <td>0.020861</td>\n",
       "      <td>5.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>64</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>0.025158</td>\n",
       "      <td>0.020862</td>\n",
       "      <td>8.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   units  stacked  dropout  dense_units      lr  batch  epochs  train_loss  \\\n",
       "0     32    False      0.2          128  0.0005    128      60    0.026194   \n",
       "1     32    False      0.2           64  0.0010    256      60    0.026305   \n",
       "2     32    False      0.0           64  0.0005    256      60    0.025342   \n",
       "3     64    False      0.2           64  0.0005    256      60    0.026378   \n",
       "4     32    False      0.2           64  0.0010    128      60    0.025975   \n",
       "5    128    False      0.2           64  0.0005    256      60    0.025820   \n",
       "6     32    False      0.2          128  0.0005    256      60    0.026573   \n",
       "7     64    False      0.4          128  0.0005    256      60    0.027030   \n",
       "8     32    False      0.4          128  0.0005    256      60    0.028095   \n",
       "9     64    False      0.0          128  0.0010    256      60    0.025158   \n",
       "\n",
       "   val_loss  time_s  \n",
       "0  0.020804    6.34  \n",
       "1  0.020811    5.02  \n",
       "2  0.020820    4.99  \n",
       "3  0.020838    7.71  \n",
       "4  0.020841    8.11  \n",
       "5  0.020842   22.39  \n",
       "6  0.020849    4.80  \n",
       "7  0.020854    9.49  \n",
       "8  0.020861    5.29  \n",
       "9  0.020862    8.61  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mejor combinación encontrada:\n",
      "{'units': 32, 'stacked': False, 'dropout': 0.2, 'dense_units': 128, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "Mejor val_loss: 0.020804\n"
     ]
    }
   ],
   "source": [
    "import itertools, time\n",
    "\n",
    "param_grid = {\n",
    "    \"units\": [32, 64, 128],\n",
    "    \"stacked\": [False, True],\n",
    "    \"dropout\": [0.0, 0.2, 0.4],\n",
    "    \"dense_units\": [64, 128],\n",
    "    \"lr\": [1e-3, 5e-4],\n",
    "    \"batch\": [128, 256],\n",
    "    \"epochs\": [60],\n",
    "}\n",
    "\n",
    "keys = list(param_grid.keys())\n",
    "combos = list(itertools.product(*[param_grid[k] for k in keys]))\n",
    "\n",
    "results = []\n",
    "print(f\"Probando {len(combos)} combinaciones...\")\n",
    "\n",
    "best = {\"val_loss\": np.inf, \"combo\": None, \"hist\": None}\n",
    "\n",
    "for i, combo in enumerate(combos, 1):\n",
    "    params = dict(zip(keys, combo))\n",
    "    print(f\"\\n[{i}/{len(combos)}] {params}\")\n",
    "\n",
    "    model = build_lstm_model(\n",
    "        units=params[\"units\"],\n",
    "        stacked=params[\"stacked\"],\n",
    "        dropout=params[\"dropout\"],\n",
    "        dense_units=params[\"dense_units\"],\n",
    "        lr=params[\"lr\"],\n",
    "    )\n",
    "\n",
    "    t0 = time.time()\n",
    "    hist = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=params[\"epochs\"],\n",
    "        batch_size=params[\"batch\"],\n",
    "        callbacks=[es, rlr],\n",
    "        verbose=0\n",
    "    )\n",
    "    dt = time.time() - t0\n",
    "\n",
    "    val_loss = float(min(hist.history[\"val_loss\"]))\n",
    "    train_loss = float(min(hist.history[\"loss\"]))\n",
    "    results.append({\n",
    "        **params,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"time_s\": round(dt, 2)\n",
    "    })\n",
    "\n",
    "    if val_loss < best[\"val_loss\"]:\n",
    "        best = {\"val_loss\": val_loss, \"combo\": params, \"hist\": hist.history}\n",
    "\n",
    "# DataFrame con ranking de combinaciones\n",
    "hp_results = pd.DataFrame(results).sort_values(\"val_loss\", ascending=True).reset_index(drop=True)\n",
    "display(hp_results.head(10)) \n",
    "\n",
    "print(\"\\nMejor combinación encontrada:\")\n",
    "print(best[\"combo\"])\n",
    "print(f\"Mejor val_loss: {best['val_loss']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e170a1",
   "metadata": {},
   "source": [
    "## 8. Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1642f182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo final con mejores hiperparámetros:\n",
      " {'units': 32, 'stacked': False, 'dropout': 0.2, 'dense_units': 128, 'lr': 0.0005, 'batch': 128, 'epochs': 60}\n",
      "Epoch 1/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0534\n",
      "Epoch 2/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0130\n",
      "Epoch 3/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0100\n",
      "Epoch 4/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0083\n",
      "Epoch 5/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0071\n",
      "Epoch 6/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0063\n",
      "Epoch 7/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0056\n",
      "Epoch 8/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0050\n",
      "Epoch 9/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0047\n",
      "Epoch 10/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0043\n",
      "Epoch 11/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0041\n",
      "Epoch 12/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0040\n",
      "Epoch 13/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0039\n",
      "Epoch 14/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0037\n",
      "Epoch 15/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0037\n",
      "Epoch 16/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0036\n",
      "Epoch 17/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0035\n",
      "Epoch 18/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0035\n",
      "Epoch 19/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0034\n",
      "Epoch 20/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0034\n",
      "Epoch 21/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0033\n",
      "Epoch 22/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0033\n",
      "Epoch 23/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0033\n",
      "Epoch 24/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0032\n",
      "Epoch 25/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0032\n",
      "Epoch 26/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0032\n",
      "Epoch 27/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0032\n",
      "Epoch 28/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0031\n",
      "Epoch 29/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0031\n",
      "Epoch 30/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0031\n",
      "Epoch 31/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0031\n",
      "Epoch 32/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0030\n",
      "Epoch 33/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0030\n",
      "Epoch 34/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.0030\n",
      "Epoch 35/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - loss: 0.0030\n",
      "Epoch 36/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - loss: 0.0030\n",
      "Epoch 37/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - loss: 0.0030\n",
      "Epoch 38/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 0.0030\n",
      "Epoch 39/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 0.0029\n",
      "Epoch 40/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.0029\n",
      "Epoch 41/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0029\n",
      "Epoch 42/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0029\n",
      "Epoch 43/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0029\n",
      "Epoch 44/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0029\n",
      "Epoch 45/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0028\n",
      "Epoch 46/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0029\n",
      "Epoch 47/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0028\n",
      "Epoch 48/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0028\n",
      "Epoch 49/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0028\n",
      "Epoch 50/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0028\n",
      "Epoch 51/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0028\n",
      "Epoch 52/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0028\n",
      "Epoch 53/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0028\n",
      "Epoch 54/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0028\n",
      "Epoch 55/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0028\n",
      "Epoch 56/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0028\n",
      "Epoch 57/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0028\n",
      "Epoch 58/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0028\n",
      "Epoch 59/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0028\n",
      "Epoch 60/60\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0027\n",
      "\n",
      "Muestra de pronósticos (multistep):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_pred_t+1</th>\n",
       "      <th>y_pred_t+2</th>\n",
       "      <th>y_pred_t+3</th>\n",
       "      <th>y_true_t+1</th>\n",
       "      <th>y_true_t+2</th>\n",
       "      <th>y_true_t+3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.472641</td>\n",
       "      <td>0.285963</td>\n",
       "      <td>0.175158</td>\n",
       "      <td>0.496506</td>\n",
       "      <td>0.354297</td>\n",
       "      <td>0.167954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.261710</td>\n",
       "      <td>0.192672</td>\n",
       "      <td>0.222727</td>\n",
       "      <td>0.354297</td>\n",
       "      <td>0.167954</td>\n",
       "      <td>0.187569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.197135</td>\n",
       "      <td>0.266573</td>\n",
       "      <td>-0.101629</td>\n",
       "      <td>0.167954</td>\n",
       "      <td>0.187569</td>\n",
       "      <td>-0.140983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.449907</td>\n",
       "      <td>0.268435</td>\n",
       "      <td>0.172088</td>\n",
       "      <td>0.490977</td>\n",
       "      <td>0.323377</td>\n",
       "      <td>0.143437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.251673</td>\n",
       "      <td>0.178526</td>\n",
       "      <td>0.224042</td>\n",
       "      <td>0.323377</td>\n",
       "      <td>0.143437</td>\n",
       "      <td>0.363477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.202094</td>\n",
       "      <td>0.278771</td>\n",
       "      <td>-0.150915</td>\n",
       "      <td>0.143437</td>\n",
       "      <td>0.363477</td>\n",
       "      <td>-0.137268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.425171</td>\n",
       "      <td>0.251030</td>\n",
       "      <td>0.174483</td>\n",
       "      <td>0.397307</td>\n",
       "      <td>0.298165</td>\n",
       "      <td>0.150192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.264208</td>\n",
       "      <td>0.172248</td>\n",
       "      <td>0.218282</td>\n",
       "      <td>0.298165</td>\n",
       "      <td>0.150192</td>\n",
       "      <td>0.233057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.202146</td>\n",
       "      <td>0.256811</td>\n",
       "      <td>-0.191617</td>\n",
       "      <td>0.150192</td>\n",
       "      <td>0.233057</td>\n",
       "      <td>-0.185706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.430893</td>\n",
       "      <td>0.254424</td>\n",
       "      <td>0.166359</td>\n",
       "      <td>0.413102</td>\n",
       "      <td>0.107553</td>\n",
       "      <td>0.183329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y_pred_t+1  y_pred_t+2  y_pred_t+3  y_true_t+1  y_true_t+2  y_true_t+3\n",
       "0    0.472641    0.285963    0.175158    0.496506    0.354297    0.167954\n",
       "1    0.261710    0.192672    0.222727    0.354297    0.167954    0.187569\n",
       "2    0.197135    0.266573   -0.101629    0.167954    0.187569   -0.140983\n",
       "3    0.449907    0.268435    0.172088    0.490977    0.323377    0.143437\n",
       "4    0.251673    0.178526    0.224042    0.323377    0.143437    0.363477\n",
       "5    0.202094    0.278771   -0.150915    0.143437    0.363477   -0.137268\n",
       "6    0.425171    0.251030    0.174483    0.397307    0.298165    0.150192\n",
       "7    0.264208    0.172248    0.218282    0.298165    0.150192    0.233057\n",
       "8    0.202146    0.256811   -0.191617    0.150192    0.233057   -0.185706\n",
       "9    0.430893    0.254424    0.166359    0.413102    0.107553    0.183329"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert 'hp_results' in globals() and len(hp_results) > 0, \"Primero ejecuta la celda de ajuste de hiperparámetros.\"\n",
    "assert 'build_lstm_model' in globals(), \"No se encontró build_lstm_model (defínela en la celda del ajuste).\"\n",
    "\n",
    "best_params = hp_results.iloc[0][[\"units\",\"stacked\",\"dropout\",\"dense_units\",\"lr\",\"batch\",\"epochs\"]].to_dict()\n",
    "print(\"Entrenando modelo final con mejores hiperparámetros:\\n\", best_params)\n",
    "\n",
    "X_tr_full = np.concatenate([X_train, X_val], axis=0)\n",
    "y_tr_full = np.concatenate([y_train, y_val], axis=0)\n",
    "\n",
    "final_model = build_lstm_model(\n",
    "    units=int(best_params[\"units\"]),\n",
    "    stacked=bool(best_params[\"stacked\"]),\n",
    "    dropout=float(best_params[\"dropout\"]),\n",
    "    dense_units=int(best_params[\"dense_units\"]),\n",
    "    lr=float(best_params[\"lr\"]),\n",
    "    output_size=HORIZON\n",
    ")\n",
    "\n",
    "final_model.fit(\n",
    "    X_tr_full, y_tr_full,\n",
    "    epochs=int(best_params[\"epochs\"]),\n",
    "    batch_size=int(best_params[\"batch\"]),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred_test = final_model.predict(X_test, verbose=0)\n",
    "\n",
    "# DataFrame con columnas por mes pronosticado\n",
    "cols_pred = [f\"y_pred_t+{i+1}\" for i in range(HORIZON)]\n",
    "forecast_df = pd.DataFrame(y_pred_test, columns=cols_pred)\n",
    "\n",
    "if 'y_test' in globals() and y_test is not None and getattr(y_test, \"ndim\", 1) == 2 and y_test.shape[1] == HORIZON:\n",
    "    for i in range(HORIZON):\n",
    "        forecast_df[f\"y_true_t+{i+1}\"] = y_test[:, i]\n",
    "\n",
    "print(\"\\nMuestra de pronósticos (multistep):\")\n",
    "display(forecast_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d33a7c0",
   "metadata": {},
   "source": [
    "# Task 2 - Teoría\n",
    "\n",
    "### 1. ¿Cuál es el problema del gradiente de fuga en las redes LSTM y cómo afecta la efectividad de LSTM para el pronóstico de series temporales? \n",
    "El problema del gradiente de fuga en las redes LSTM, es que aunque las LSTM están diseñadas para mitigarlo no son inmunes, ya que en LSTM el estado de la celda va de esta manera:\n",
    "\n",
    "$$\n",
    "c_t = f_t \\odot c_{t-1} + i_t \\odot g_t\n",
    "$$\n",
    "\n",
    "Y en la retropropagación a través del tiempo hace que la gradiente hacia:\n",
    "$$\n",
    "c_{t-1}\n",
    "$$\n",
    "\n",
    "se multiplique por el producto de las compuertas del olvido, entonces el resultado del producto tiende a 0 cuando la secuencia es larga o las compuertas se cierran más, entonces el gradiente se desvanece y si fuera mayor que 1 explotaría. Y eso pasa en la práctuca ya que se puede dar la saturación de sigmoides, sesgo del olvido mal iniciado, secuencias largas y normalización o escala pobre de entradas. Y esto afecta al pronóstico, porque no capta dependencias de largo plazo, tiene predicciones que sub-reaccionan a las señales antiguas, tiene una convergencia lenta y es inestable, ya que a varios pasos el error crece y el modelo olvida el contexto útil.\n",
    "\n",
    "\n",
    "### 2. ¿Cómo se aborda la estacionalidad en los datos de series temporales cuando se utilizan LSTM para realizar pronósticos y qué papel juega la diferenciación en el proceso? \n",
    "\n",
    "El manejo de estacionalidad en LSTM se puede modelar la estacionalidad como caracteristicas o al quitarla y recomponerla. Ya que, la diferenciación lo que hacec es estabilizar y facilitar el aprendizaje, pero requiere de reintegrar y puede sobreactuar si la estacionalidad no es rígida. En el caso que la estacionalidad sea fuerte o regular es mejor deseasonalizarla, quitarla al entrenar el LSTM sobre la serie limpua y reaplicar al estacionalidad al final. Mientras que si es informativa para el modelo, es mejor agregarlas como features exógenas.\n",
    "\n",
    "\n",
    "\n",
    "### 3. ¿Cuál es el concepto de \"tamaño de ventana\" en el pronóstico de series temporales con LSTM y cómo afecta la elección del tamaño de ventana a la capacidad del modelo para capturar patrones a corto y largo plazo? \n",
    "\n",
    "El concepto de tamaño de ventana o tambien llamado lookback o W, es cuantos pasos del pasado se le entrega al modelo para predecir el futuro. Y el W si es una ventana corta o pequeña, tiende a patrones de corto plazo, entonces este aprende rápidamente señales locales, pero no ve ciclos largos. Mientras que si es una ventana larga o grande, tiende a patrones de largo plazo, estos pueden capturar tendencias y estacionalidades largas si están presentes en la ventana, tiene parámetros más efectivos, mayor varianza/overfitting, y el entrenamiento es más lengo. Por el desvanecimiento de gradientes si la información útil está muy lejos el LSTM no puede aprovechar todo. Por lo tanto para evaluar el tamaño de la ventana y saber como ajustarla es imporante analizar la serie y su ciclo. Si la H es grande, los datos limitados para evitar W demasiado grandes y si tiene múltiples ciclos. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b528321c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deepvenv)",
   "language": "python",
   "name": "deepvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
